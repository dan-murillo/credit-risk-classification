{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported all the dependencies and modules needed:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the Data into Training and Testing Sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read the `lending_data.csv` data from the `Resources` folder into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_size</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>borrower_income</th>\n",
       "      <th>debt_to_income</th>\n",
       "      <th>num_of_accounts</th>\n",
       "      <th>derogatory_marks</th>\n",
       "      <th>total_debt</th>\n",
       "      <th>loan_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10700.0</td>\n",
       "      <td>7.672</td>\n",
       "      <td>52800</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8400.0</td>\n",
       "      <td>6.692</td>\n",
       "      <td>43600</td>\n",
       "      <td>0.311927</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9000.0</td>\n",
       "      <td>6.963</td>\n",
       "      <td>46100</td>\n",
       "      <td>0.349241</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10700.0</td>\n",
       "      <td>7.664</td>\n",
       "      <td>52700</td>\n",
       "      <td>0.430740</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10800.0</td>\n",
       "      <td>7.698</td>\n",
       "      <td>53000</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>23000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_size  interest_rate  borrower_income  debt_to_income  num_of_accounts  \\\n",
       "0    10700.0          7.672            52800        0.431818                5   \n",
       "1     8400.0          6.692            43600        0.311927                3   \n",
       "2     9000.0          6.963            46100        0.349241                3   \n",
       "3    10700.0          7.664            52700        0.430740                5   \n",
       "4    10800.0          7.698            53000        0.433962                5   \n",
       "\n",
       "   derogatory_marks  total_debt  loan_status  \n",
       "0                 1       22800            0  \n",
       "1                 0       13600            0  \n",
       "2                 0       16100            0  \n",
       "3                 1       22700            0  \n",
       "4                 1       23000            0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file from the Resources folder into a Pandas DataFrame:\n",
    "file_path = Path('Resources/lending_data.csv')\n",
    "lending_df = pd.read_csv(file_path)\n",
    "\n",
    "# Reviewed the DataFrame:\n",
    "lending_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Created the labels set (`y`)  from the “loan_status” column, and then create the features (`X`) DataFrame from the remaining columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separated the data into labels and features:\n",
    "# First, separated the 'y' variable, which was the labels:\n",
    "y = lending_df['loan_status']\n",
    "\n",
    "# Then, separated the 'X' variable, which was the features:\n",
    "X = lending_df.drop(columns='loan_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: loan_status, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviewed the 'y' variable Series:\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_size</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>borrower_income</th>\n",
       "      <th>debt_to_income</th>\n",
       "      <th>num_of_accounts</th>\n",
       "      <th>derogatory_marks</th>\n",
       "      <th>total_debt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10700.0</td>\n",
       "      <td>7.672</td>\n",
       "      <td>52800</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8400.0</td>\n",
       "      <td>6.692</td>\n",
       "      <td>43600</td>\n",
       "      <td>0.311927</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9000.0</td>\n",
       "      <td>6.963</td>\n",
       "      <td>46100</td>\n",
       "      <td>0.349241</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10700.0</td>\n",
       "      <td>7.664</td>\n",
       "      <td>52700</td>\n",
       "      <td>0.430740</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10800.0</td>\n",
       "      <td>7.698</td>\n",
       "      <td>53000</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>23000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_size  interest_rate  borrower_income  debt_to_income  num_of_accounts  \\\n",
       "0    10700.0          7.672            52800        0.431818                5   \n",
       "1     8400.0          6.692            43600        0.311927                3   \n",
       "2     9000.0          6.963            46100        0.349241                3   \n",
       "3    10700.0          7.664            52700        0.430740                5   \n",
       "4    10800.0          7.698            53000        0.433962                5   \n",
       "\n",
       "   derogatory_marks  total_debt  \n",
       "0                 1       22800  \n",
       "1                 0       13600  \n",
       "2                 0       16100  \n",
       "3                 1       22700  \n",
       "4                 1       23000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviewed the 'X' variable DataFrame:\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Checked the balance of the labels variable (`y`) by using the `value_counts` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    75036\n",
       "1     2500\n",
       "Name: loan_status, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checked the balance of the target values:\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Split the data into training and testing datasets by using `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data using 'train_test_split', seshaped the 'y' variable to better fit the data into the function, and\n",
    "# assigned a 'random_state' of 1:\n",
    "y = y.values.reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Logistic Regression Model with the Original Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 1: Fitted a logistic regression model by using the training data (`X_train` and `y_train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a 'StandardScaler' instance, fitted it, and scaled the data to standardise the features, remove the mean,\n",
    "# and scale to unit variance, so that the 'LogisticRegression' behaves better:\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Instantiated the 'Logistic Regression' model, and assigned a 'random_state' parameter of 1 to the model:\n",
    "classifier = LogisticRegression(random_state=1)\n",
    "\n",
    "# Fitted the model using the training data and used the 'ravel()' function to fit the original array:\n",
    "lr_model = classifier.fit(X_train_scaled, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Saved the predictions on the testing data labels by using the testing feature data (`X_test`) and the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Made a prediction using the testing data:\n",
    "testing_predictions = lr_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluated the model’s performance by doing the following:\n",
    "\n",
    "* Calculated the accuracy score of the model,\n",
    "\n",
    "* Generated a confusion matrix,\n",
    "\n",
    "* Printed the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the model is: 0.9889115309798473\n"
     ]
    }
   ],
   "source": [
    "# Printed the 'balanced_accuracy' score of the model:\n",
    "balanced_acc_score = balanced_accuracy_score(y_test, testing_predictions)\n",
    "print(f\"The accuracy score of the model is: {balanced_acc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>18652</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>10</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0        18652          113\n",
       "Actual 1           10          609"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generated a confusion matrix for the model:\n",
    "test_cm = confusion_matrix(y_test, testing_predictions)\n",
    "test_cm_df = pd.DataFrame(test_cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "print('Confusion matrix:')\n",
    "display(test_cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     18765\n",
      "           1       0.84      0.98      0.91       619\n",
      "\n",
      "    accuracy                           0.99     19384\n",
      "   macro avg       0.92      0.99      0.95     19384\n",
      "weighted avg       0.99      0.99      0.99     19384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printed the classification report for the model:\n",
    "testing_report = classification_report(y_test, testing_predictions)\n",
    "print('Testing classification report:')\n",
    "print(testing_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Answered the following question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How well does the logistic regression model predict both the <font color=green>`0` (healthy loan)</font> and <font color=darkred>`1` (high-risk loan)</font> labels?\n",
    "\n",
    "**Answer:** **In summary**, the above logistic regression model works *very well* to classify both <font color=green>'healthy loans'</font> and <font color=darkred>'high-risk loans'</font>. The model is as accurate as it can be for classifying <font color=green>'healthy loans'</font> and very accurate for <font color=darkred>'high-risk loans'</font> —based on the F1 scores, which are <font color=green>100%</font> and <font color=darkred>91%</font> respectively. The classifications made by the model are very likely correct because the precision scores are very high —<font color=green>100%</font> for <font color=green>0's</font> and <font color=darkred>84%</font> for <font color=darkred>1's</font>—, and the model correctly finds nearly all the true <font color=darkred>'high-risk loans'</font> there are, as the recall scores were extremely high too —<font color=green>99%</font> for <font color=green>0's</font> and <font color=darkred>98%</font> for <font color=darkred>1's</font>.\n",
    "\n",
    "The following is a **detailed** explanation of the preceding paragraph. The goal of the model is to classify the right loans into the right categories. Although the *accuracy* score seems very high, it does not truly depict accuracy because there is a high class imbalance between the <font color=green>0's</font> and <font color=darkred>1's</font> —only <font color=darkred>3.2%</font> of the target values were <font color=darkred>'high-risk loans'</font> whereas <font color=green>the rest</font> were <font color=green>'healthy loans'</font>. Therefore, that score cannot correctly distinguish between false <font color=green>'healthy loans'</font> and false <font color=darkred>'high-risk loans'</font>.\n",
    "\n",
    "When there is such a class imbalance, it is better to look at the model's precision, recall, and F1 score. The *precision* score reveals how precise a model is. The score of <font color=green>'healthy loans'</font> is as high as it can be, which means that the model correctly predicts <font color=green>100%</font> of the <font color=green>'healthy loans'</font>. On the other hand, the precision score of <font color=darkred>'high-risk loans'</font> was <font color=darkred>84%</font>, which, although not perfect, is still a high score. Although being wrong 14% of the time can sound alarming, in our situation it is not because the costs of mis-classifying a <font color=green>'healthy loan'</font> as a <font color=darkred>'high-risk loan'</font> are not nearly as high as mis-classifying a <font color=darkred>'high-risk loan'</font> as a <font color=green>'healthy loan'</font>. The costs of the latter situation are better explained by the recall score.\n",
    "\n",
    "The *recall* score calculates how many of the actual <font color=darkred>'high-risk loans'</font> the model capture as <font color=darkred>'high-risk loans'</font>. Hence, the higher the score, the lower the costs of mis-classifying <font color=darkred>this type</font> of loans. The recall score of <font color=darkred>'high-risk loans'</font> was <font color=darkred>98%</font>, which is extremely high. That is very desirable because the model correctly identified nearly all the <font color=darkred>'high-risk loans'</font>. Therefore, the probabilities of mis-classifying <font color=darkred>this type</font> of loans is extremely low.\n",
    "\n",
    "Finally, the *F1 score* is the harmonic mean of precision and recall. It is an alternative to the arithmetic mean, and it is used when working with rates. The F1 score of <font color=green>'healthy loans'</font> is <font color=green>100%</font> —as high as it can be—, while the score of <font color=darkred>'high-risk loans'</font> is <font color=darkred>91%</font> —very high. Those F1 scores show that the model is very accurate, and thus, performs very well, even for <font color=darkred>'high-risk loans'</font> almost all of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a Logistic Regression Model with Resampled Training Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Used the `RandomOverSampler` module from the imbalanced-learn library to resample the data and make sure that the labels had an equal number of data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the random oversampler model, and assigned a 'random_state' parameter of 1 to the model:\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "\n",
    "# Fitted the original training data to the 'random_oversampler' model:\n",
    "X_train, y_train = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 75036, 1: 75036})\n"
     ]
    }
   ],
   "source": [
    "# Counted the distinct values of the resampled labels data:\n",
    "print('Resampled dataset shape %s' % Counter(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Used the `LogisticRegression` classifier and the resampled data to fit the model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a 'StandardScaler' instance, fitted it, and scaled the data to standardise the features, remove the mean,\n",
    "# and scale to unit variance, so that the 'LogisticRegression' behaves better:\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Instantiated the 'Logistic Regression' model, and assigned a 'random_state' parameter of 1 to the model:\n",
    "classifier = LogisticRegression(random_state=1)\n",
    "\n",
    "# Fitted the model using the resampled training data, and used the 'ravel()' function to fit the original array:\n",
    "lr_model = classifier.fit(X_train_scaled, y_train.ravel())\n",
    "\n",
    "# Made a prediction using the testing data:\n",
    "testing_predictions = lr_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluated the model’s performance by doing the following:\n",
    "\n",
    "* Calculated the accuracy score of the model,\n",
    "\n",
    "* Generated a confusion matrix,\n",
    "\n",
    "* Printed the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of the model is: 0.9934916041318802\n"
     ]
    }
   ],
   "source": [
    "# Printed the 'balanced_accuracy' score of the model:\n",
    "balanced_acc_score = balanced_accuracy_score(y_test, testing_predictions)\n",
    "print(f\"The accuracy score of the model is: {balanced_acc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>18642</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>4</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0        18642          123\n",
       "Actual 1            4          615"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generated a confusion matrix for the model:\n",
    "test_cm = confusion_matrix(y_test, testing_predictions)\n",
    "test_cm_df = pd.DataFrame(test_cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "print('Confusion matrix:')\n",
    "display(test_cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     18765\n",
      "           1       0.83      0.99      0.91       619\n",
      "\n",
      "    accuracy                           0.99     19384\n",
      "   macro avg       0.92      0.99      0.95     19384\n",
      "weighted avg       0.99      0.99      0.99     19384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printed the classification report for the model:\n",
    "testing_report = classification_report(y_test, testing_predictions)\n",
    "print('Testing classification report:')\n",
    "print(testing_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Answer the following question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How well does the logistic regression model, fit with oversampled data, predict both the `0` (healthy loan) and `1` (high-risk loan) labels?\n",
    "\n",
    "**Answer:** **In summary**, oversampling the <font color=darkred>'high-risk loans'</font> improved the performance of the previous logistic regression model, which makes this improved model the safest to make decisions on <font color=darkred>that type</font> of loans. In general, this model works *very well* to classify both <font color=green>'healthy loans'</font> and <font color=darkred>'high-risk loans'</font>, as it is 99% accurate. The classifications made by the model are very likely correct because the precision scores are very high —<font color=green>100%</font> for <font color=green>0's</font> and <font color=darkred>83%</font> for <font color=darkred>1's</font>—, and this model also correctly finds nearly all the true <font color=darkred>'high-risk loans'</font> there are, as the recall score was extremely high: 99%.\n",
    "\n",
    "The following is a **detailed** explanation of the preceding paragraph. Due to the oversampling of <font color=darkred>'high-risk loans'</font>, both classifications are now balanced, so the *accuracy* score can be trusted. It is very high —99%—, which means that this improved model is extremely accurate.\n",
    "\n",
    "This improved model's precision and recall scores can shed more light on how well it performs. The *precision* score of <font color=green>'healthy loans'</font> is still as high as it can be, which means that the model correctly predicts <font color=green>100%</font> of <font color=green>that type</font> of loans. However, the precision score of <font color=darkred>'high-risk loans'</font> decreased to <font color=darkred>83%</font>, which is still a high score. Again, the costs of mis-classifying a <font color=darkred>'high-risk loan'</font> as a <font color=green>'healthy loan'</font> are more important, so the recall score should have more weight.\n",
    "\n",
    "The *recall* score of <font color=darkred>'high-risk loans'</font> in this improved model was <font color=darkred>99%</font>, which is 1% higher than the previous model's recall score. That makes this model ideal for correctly identifying <font color=darkred>'high-risk loans'</font> and reduce mis-classification costs as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonFiona] *",
   "language": "python",
   "name": "conda-env-PythonFiona-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
